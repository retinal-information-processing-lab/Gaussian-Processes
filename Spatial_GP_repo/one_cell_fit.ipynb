{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0 (from utils.py)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '/home/idv-eqs8-pza/IDV_code/Variational_GP/spatial_GP')\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "TORCH_DTYPE = torch.float32 #NB: Basically all of the matrices in Spatial_GP have 1.e-7 added to the diagonal, to be changed if we want to use float64\n",
    "# Set the default dtype to float32\n",
    "torch.set_default_dtype(TORCH_DTYPE)\n",
    "\n",
    "\n",
    "\n",
    "# with open('data/processed_data.pkl', 'wb') as f:\n",
    "#     pickle.save(f)\n",
    "\n",
    "# To compare with samuale use:\n",
    "# save_pickle('pietro_data', **{'K_pietro':K, 'K_tilde_pietro':K_tilde})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_xtilde = False # If True, xtilde (inducing points) are chosen randomly, if False, xtilde is chosen from the first ntilde images\n",
    "noise       = False # If True, noise is added to xtilde (used to avoid inducing points being too close to each other in their space)\n",
    "rescale     = False # If True, X is rescaled between -1 and 1\n",
    "cellid      = 1   # Choose cell\n",
    "ntilde      = 50   # Number of xtilde\n",
    "kernfun     = acosker_samu # Choose kernel function\n",
    "\n",
    "Nmstep  = 10\n",
    "Nestep  = 10\n",
    "Maxiter = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset and preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda:0\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#  Factor 1/2 is removed from acosker\n",
    "##################\n",
    "\n",
    "# Set the device \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "torch.set_default_dtype(TORCH_DTYPE)\n",
    "torch.set_default_device(device)\n",
    "print(f'Device is: {device}')\n",
    "\n",
    "\n",
    "    \n",
    "# Open the .pkl dataset file for reading in binary mode (rb)\n",
    "with open('/home/idv-eqs8-pza/IDV_code/Variational_GP/spatial_GP/Data/data2_41mixed_tr28.pkl', 'rb') as file:\n",
    "    # Load the data from the file\n",
    "    loaded_data = pickle.load(file)\n",
    "    # loaded_data is a Dataset object from module Data with attributes \"images_train, _val, _test\" as well as responses\n",
    "\n",
    "X_train = torch.tensor(loaded_data.images_train).to(device) #shape (2910,108,108,1) where 108 is the number of pixels. 2910 is the amount of training data\n",
    "X_val   = torch.tensor(loaded_data.images_val).to(device)\n",
    "X_test  = torch.tensor(loaded_data.images_test).to(device) # shape (30,108,108,1) # nimages, npx, npx\n",
    "\n",
    "R_train = torch.tensor(loaded_data.responses_train).to(device, dtype=TORCH_DTYPE) #shape (2910,41) 2910 is the amount of training data, 41 is the number of cells\n",
    "R_val   = torch.tensor(loaded_data.responses_val).to(device, dtype=TORCH_DTYPE)\n",
    "R_test   = torch.tensor(loaded_data.responses_test).to(device, dtype=TORCH_DTYPE) # shape (30,30,42) 30 repetitions, 30 images, 42 cells\n",
    "\n",
    "# Concatenate Xtrain and Xval if not using validation set \n",
    "X = torch.cat( (X_train, X_val), axis=0,) #shape (3160,108,108,1)\n",
    "R = torch.cat( (R_train, R_val), axis=0,)\n",
    "# X = X_train\n",
    "# R = R_train\n",
    "\n",
    "# Rescale X to be between -1 and 1\n",
    "if rescale == True:\n",
    "    X = (X - X.min()) / (X.max() - X.min()) * 2 - 1\n",
    "\n",
    "# Choose a cell\n",
    "r = R[:,cellid] # shape (nt,) where nt is the number of trials\n",
    "\n",
    "# Reshape images to 1D vector\n",
    "n_px_side = X.shape[1] # 108   \n",
    "X = torch.reshape(X, ( X.shape[0], X.shape[1]*X.shape[2])) # shape (n x , 11664)=(nt, nx)\n",
    "# If X_val is being used, reshape it\n",
    "# X_val = torch.reshape(X_val, ( X_val.shape[0], X_val.shape[1]*X_val.shape[2])) # shape (n x val, 11664)=(nt, nx)\n",
    "\n",
    "if rand_xtilde == True:\n",
    "    torch.manual_seed(2024)\n",
    "    indices = torch.randint(0, X.shape[0], (ntilde,))\n",
    "else:\n",
    "    indices = torch.arange(0,ntilde, dtype=torch.int64)\n",
    "if noise == True:\n",
    "    xtilde = X[indices,:] + 1e-7*torch.rand(X[indices,:].shape)*2 - 1.e-7\n",
    "else:\n",
    "    xtilde = X[indices,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated sigma_0 to 1.0\n",
      "updated eps_0x to 0.0\n",
      "updated eps_0y to 0.0\n",
      "updated -2log2beta to 4.8068528175354\n",
      "updated -log2rho2 to 4.3068528175354\n",
      "updated Amp to 1.0\n",
      " Before overloading\n",
      " Hyperparameters have been SET as  : beta = 0.05856070, rho = 0.02928035\n",
      " Samuele hyperparameters           : logbetasam = 4.9822, logrhosam = 7.0617\n",
      "\n",
      " After overloading\n",
      " Dict of learnable hyperparameters : sigma_0 = 1.00000000, eps_0x = 0.00000000, eps_0y = 0.00000000, -2log2beta = 4.80685282, -log2rho2 = 4.30685282, Amp = 1.00000000\n",
      " Hyperparameters from the logexpr  : beta = 0.04520382, rho = 0.08208501\n",
      " Samuele hyperparameters           : logbetasam = 5.5000, logrhosam = 5.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If one wants to compare the hyperparemeters set in Samuele's code:\n",
    "logsigma_0 = torch.tensor(0) # Samuele's code set the log of sigma\n",
    "\n",
    "logbetaexpr = fromlogbetasam_to_logbetaexpr( logbetasam=torch.tensor(5.5) )# Logbetaexpr in this code is equal to logbeta in Samuele's code. Samuele's code set logbeta to 5.5\n",
    "\n",
    "logrhoexpr  = fromlogrhosam_to_logrhoexpr( logrhosam=torch.tensor(5)) \n",
    "\n",
    "Amp = torch.tensor(1.0) # Samuele's code set Amp to 1 does not learn it (absent in the code)\n",
    "\n",
    "theta = {'sigma_0': torch.exp(logsigma_0), 'eps_0x':torch.tensor(0.0), 'eps_0y':torch.tensor(0.0), '-2log2beta': logbetaexpr, '-log2rho2': logrhoexpr, 'Amp': Amp }\n",
    "\n",
    "for key, value in theta.items():\n",
    "    theta[key] = value.requires_grad_()\n",
    "\n",
    "hyperparams_tuple = generate_theta( x=X, r=r, n_px_side=n_px_side, display=True, **theta)\n",
    "\n",
    "\n",
    "args = {\n",
    "        'ntilde':  ntilde,\n",
    "        'Maxiter': Maxiter,\n",
    "        'Nmstep':  Nmstep,\n",
    "        'Nestep':  Nestep,\n",
    "        'kernfun': kernfun,\n",
    "        'n_px_side': n_px_side,\n",
    "        'display_hyper': True,\n",
    "        'display_prog':  True,\n",
    "        'hyperparams_tuple': hyperparams_tuple,\n",
    "        'xtilde': xtilde,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GP step loss:   0%|          | 0/10 [00:00<?, ?it/s]/home/idv-eqs8-pza/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/_device.py:77: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/aten/src/ATen/native/TensorShape.cpp:3614.)\n",
      "  return func(*args, **kwargs)\n",
      "EE step loss: 2591.4062: 100%|██████████| 10/10 [00:03<00:00,  2.94it/s]\n",
      "EE step loss: 1897.2102: 100%|██████████| 10/10 [00:03<00:00,  3.23it/s]\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "torch.linalg.solve: The solver failed because the input matrix is singular.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m theta, f_params, m, V, C, mask, K_tilde_inv, K_tilde, values_track \u001b[38;5;241m=\u001b[39m varGP(X, r, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/IDV_code/Variational_GP/Gaussian-Processes/Spatial_GP_repo/utils.py:1362\u001b[0m, in \u001b[0;36mvarGP\u001b[0;34m(x, r, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                   theta[key]\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mdlogmarginal[key]\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m-> 1362\u001b[0m     loss \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(closure) \n\u001b[1;32m   1364\u001b[0m     progbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGP step loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1365\u001b[0m progbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/optim/lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 438\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(closure())\n\u001b[1;32m    439\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    440\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/IDV_code/Variational_GP/Gaussian-Processes/Spatial_GP_repo/utils.py:1335\u001b[0m, in \u001b[0;36mvarGP.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1333\u001b[0m K_tilde, dK_tilde     \u001b[38;5;241m=\u001b[39m kernfun(theta, xtilde[:,mask], xtilde[:,mask], C\u001b[38;5;241m=\u001b[39mC, dC\u001b[38;5;241m=\u001b[39mdC, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1334\u001b[0m K, dK                 \u001b[38;5;241m=\u001b[39m kernfun(theta, x[:,mask], xtilde[:,mask], C\u001b[38;5;241m=\u001b[39mC, dC\u001b[38;5;241m=\u001b[39mdC, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1335\u001b[0m K_tilde_inv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(K_tilde,torch\u001b[38;5;241m.\u001b[39meye(K_tilde\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) ) \u001b[38;5;66;03m# shape (ntilde, ntilde)\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m KKtilde_inv \u001b[38;5;241m=\u001b[39m K \u001b[38;5;241m@\u001b[39m K_tilde_inv \u001b[38;5;66;03m# shape (nt, ntilde)\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m Kvec, dKvec           \u001b[38;5;241m=\u001b[39m kernfun(theta, x[:,mask], x2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, C\u001b[38;5;241m=\u001b[39mC, dC\u001b[38;5;241m=\u001b[39mdC, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: torch.linalg.solve: The solver failed because the input matrix is singular."
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "theta, f_params, m, V, C, mask, K_tilde_inv, K_tilde, values_track = varGP(X, r, **args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "model = {\n",
    "    'theta': theta,\n",
    "    'f_params': f_params,\n",
    "    'm': m,\n",
    "    'V': V,\n",
    "    'C': C,\n",
    "    'mask': mask,\n",
    "    'K_tilde_inv': K_tilde_inv,\n",
    "    'K_tilde': K_tilde,\n",
    "    'cellid': cellid,\n",
    "    'ntilde': ntilde,\n",
    "    'Maxiter': Maxiter,\n",
    "    'Nmstep': Nmstep,\n",
    "    'Nestep': Nestep,\n",
    "    'kernfun': kernfun,\n",
    "    'values_track': values_track,\n",
    "}\n",
    "\n",
    "# save_pickle('pietro_model', **model)\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# TEMP\n",
    "# To compare to samuele I have to kep the aplitude of C fixed to 1, he does not have it\n",
    "theta['Amp'] = 1.0\n",
    "\n",
    "\n",
    "print(f'f_params: {f_params}')\n",
    "for key, value in theta.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Predict and test\n",
    "\n",
    "rtst, R_predicted, r2, sigma_r2 = test(X_test, R_test, xtilde, **model )\n",
    "\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = fig.add_gridspec(5, 5,\n",
    "            left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "            wspace=0.3, hspace=0.7)\n",
    "dt = 0.05\n",
    "time_values = dt * np.arange( len(R_predicted) )\n",
    "ax = fig.add_subplot(gs[3:, :])\n",
    "ax.plot(time_values, np.mean(rtst, axis=0) / 0.05, 'k', linewidth=1)\n",
    "\n",
    "ax.plot(time_values, R_predicted / 0.05, color='red', label='GP')\n",
    "# ax.errorbar(time_values, R_predicted / 0.05, yerr=np.sqrt(sigma2_f[:,0].cpu()) / 0.05, color='red')\n",
    "# ax.legend(['data', 'GP'], loc='upper right', fontsize=14)\n",
    "txt = f'Pietro adjusted r^2 = {r2:.2f} ± {sigma_r2:.2f} Cell: {cellid}'\n",
    "ax.set_title(f'{txt}')\n",
    "# ax.set_ylabel('Firing rate (Hz)')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "a=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
