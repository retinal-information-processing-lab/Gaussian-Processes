{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0 (from utils.py)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '/home/idv-eqs8-pza/IDV_code/Variational_GP/spatial_GP')\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "TORCH_DTYPE = torch.float64 #NB: Basically all of the matrices in Spatial_GP have 1.e-7 added to the diagonal, to be changed if we want to use float64\n",
    "# Set the default dtype to float64\n",
    "torch.set_default_dtype(TORCH_DTYPE)\n",
    "\n",
    "\n",
    "\n",
    "# with open('data/processed_data.pkl', 'wb') as f:\n",
    "#     pickle.save(f)\n",
    "\n",
    "# To compare with samuale use:\n",
    "# save_pickle('pietro_data', **{'K_pietro':K, 'K_tilde_pietro':K_tilde})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_xtilde = False # If True, xtilde (inducing points) are chosen randomly, if False, xtilde is chosen from the first ntilde images\n",
    "noise       = False # If True, noise is added to xtilde (used to avoid inducing points being too close to each other in their space)\n",
    "rescale     = False # If True, X is rescaled between -1 and 1\n",
    "cellid      = 8   # Choose cell\n",
    "ntilde      = 2000  # Number of xtilde\n",
    "kernfun     = acosker_samu # Choose kernel function\n",
    "\n",
    "Nmstep  = 0\n",
    "Nestep  = 20\n",
    "Maxiter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset and preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "\n",
    "# Set the device \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "torch.set_default_dtype(TORCH_DTYPE)\n",
    "torch.set_default_device(device)\n",
    "print(f'Device is: {device}')\n",
    "\n",
    "# Open the .pkl dataset file for reading in binary mode (rb)\n",
    "with open('/home/idv-eqs8-pza/IDV_code/Variational_GP/spatial_GP/Data/data2_41mixed_tr28.pkl', 'rb') as file:\n",
    "    # Load the data from the file\n",
    "    loaded_data = pickle.load(file)\n",
    "    # loaded_data is a Dataset object from module Data with attributes \"images_train, _val, _test\" as well as responses\n",
    "\n",
    "X_train = torch.tensor(loaded_data.images_train).to(device, dtype=TORCH_DTYPE) #shape (2910,108,108,1) where 108 is the number of pixels. 2910 is the amount of training data\n",
    "X_val   = torch.tensor(loaded_data.images_val).to(device, dtype=TORCH_DTYPE)\n",
    "X_test  = torch.tensor(loaded_data.images_test).to(device, dtype=TORCH_DTYPE) # shape (30,108,108,1) # nimages, npx, npx\n",
    "\n",
    "R_train = torch.tensor(loaded_data.responses_train).to(device, dtype=TORCH_DTYPE) #shape (2910,41) 2910 is the amount of training data, 41 is the number of cells\n",
    "R_val   = torch.tensor(loaded_data.responses_val).to(device, dtype=TORCH_DTYPE)\n",
    "R_test   = torch.tensor(loaded_data.responses_test).to(device, dtype=TORCH_DTYPE) # shape (30,30,42) 30 repetitions, 30 images, 42 cells\n",
    "\n",
    "# Concatenate Xtrain and Xval if not using validation set \n",
    "X = torch.cat( (X_train, X_val), axis=0,) #shape (3160,108,108,1)\n",
    "R = torch.cat( (R_train, R_val), axis=0,)\n",
    "# X = X_train\n",
    "# R = R_train\n",
    "\n",
    "# Rescale X to be between -1 and 1\n",
    "if rescale == True:\n",
    "    X = (X - X.min()) / (X.max() - X.min()) * 2 - 1\n",
    "\n",
    "# Choose a cell\n",
    "r = R[:,cellid] # shape (nt,) where nt is the number of trials\n",
    "\n",
    "# Reshape images to 1D vector\n",
    "n_px_side = X.shape[1] # 108   \n",
    "X = torch.reshape(X, ( X.shape[0], X.shape[1]*X.shape[2])) # shape (n x , 11664)=(nt, nx)\n",
    "# If X_val is being used, reshape it\n",
    "# X_val = torch.reshape(X_val, ( X_val.shape[0], X_val.shape[1]*X_val.shape[2])) # shape (n x val, 11664)=(nt, nx)\n",
    "\n",
    "if rand_xtilde == True:\n",
    "    torch.manual_seed(2024)\n",
    "    indices = torch.randint(0, X.shape[0], (ntilde,))\n",
    "else:\n",
    "    indices = torch.arange(0,ntilde, dtype=torch.int64)\n",
    "if noise == True:\n",
    "    xtilde = X[indices,:] + 1e-7*torch.rand(X[indices,:].shape)*2 - 1.e-7\n",
    "else:\n",
    "    xtilde = X[indices,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated sigma_0 to 1.0\n",
      "updated eps_0x to 0.0\n",
      "updated eps_0y to 0.0\n",
      "updated -2log2beta to 4.65\n",
      "updated -log2rho2 to 4.3\n",
      "updated Amp to 1.0\n",
      " Before overloading\n",
      " Hyperparameters have been SET as  : beta = 0.05856070, rho = 0.02928035\n",
      " Samuele hyperparameters           : logbetasam = 4.9822, logrhosam = 7.0617\n",
      "\n",
      " After overloading\n",
      " Dict of learnable hyperparameters : sigma_0 = 1.00000000, eps_0x = 0.00000000, eps_0y = 0.00000000, -2log2beta = 4.65000000, -log2rho2 = 4.30000000, Amp = 1.00000000\n",
      " Hyperparameters from the logexpr  : beta = 0.04889172, rho = 0.08236674\n",
      " Samuele hyperparameters           : logbetasam = 5.3431, logrhosam = 4.9931\n"
     ]
    }
   ],
   "source": [
    "# If one wants to compare the hyperparemeters set in Samuele's code:\n",
    "logsigma_0 = torch.tensor(0) # Samuele's code set the log of sigma\n",
    "\n",
    "logbetaexpr = torch.tensor(4.65)\n",
    "# logbetaexpr = fromlogbetasam_to_logbetaexpr( logbetasam=torch.tensor(5.5) )# Logbetaexpr in this code is equal to logbeta in Samuele's code. Samuele's code set logbeta to 5.5\n",
    "\n",
    "logrhoexpr = torch.tensor(4.3)\n",
    "# logrhoexpr  = fromlogrhosam_to_logrhoexpr( logrhosam=torch.tensor(5)) \n",
    "\n",
    "Amp = torch.tensor(1.0) # Samuele's code set Amp to 1 does not learn it (absent in the code)\n",
    "\n",
    "# test\n",
    "theta = {'sigma_0': torch.exp(logsigma_0), 'eps_0x':torch.tensor(0.), 'eps_0y':torch.tensor(0.), '-2log2beta': logbetaexpr, '-log2rho2': logrhoexpr, 'Amp': Amp }\n",
    "# theta = {'sigma_0': torch.tensor(1.e-6), 'eps_0x':torch.tensor(0.0), 'eps_0y':torch.tensor(0.0), '-2log2beta': logbetaexpr, '-log2rho2': logrhoexpr, 'Amp': Amp }\n",
    "\n",
    "for key, value in theta.items():\n",
    "    #test\n",
    "    # to exclude Amp from the optimization\n",
    "    # if key == 'Amp':\n",
    "        # continue\n",
    "    # if key == 'sigma_0':\n",
    "        # continue\n",
    "    # if key == '-log2rho2' or key == '-2log2beta':\n",
    "        # continue\n",
    "    # if key == 'eps_0x' or key == 'eps_0y':\n",
    "    #    continue \n",
    "    #end test\n",
    "    theta[key] = value.requires_grad_()\n",
    "\n",
    "hyperparams_tuple = generate_theta( x=X, r=r, n_px_side=n_px_side, display=True, **theta)\n",
    "\n",
    "\n",
    "# A        = torch.tensor(0.007)\n",
    "A        = torch.tensor(0.007)\n",
    "logA     = torch.log(A)\n",
    "f_params = {'logA': logA, 'lambda0':torch.tensor(0.31)}\n",
    "# f_params = {'A': torch.tensor(0.0017), 'lambda0':torch.tensor(0.3697)}\n",
    "# f_params = {'A': torch.tensor(-0.0807), 'lambda0': torch.tensor(0.3762)}\n",
    "for key, value in f_params.items():\n",
    "    f_params[key] = value.requires_grad_()\n",
    "\n",
    "args = {\n",
    "        'ntilde':  ntilde,\n",
    "        'Maxiter': Maxiter,\n",
    "        'Nmstep':  Nmstep,\n",
    "        'Nestep':  Nestep,\n",
    "        'kernfun': kernfun,\n",
    "        'n_px_side': n_px_side,\n",
    "        'display_prog':  False,\n",
    "        'hyperparams_tuple': hyperparams_tuple,\n",
    "        'f_params': f_params,\n",
    "        'xtilde': xtilde,\n",
    "\n",
    "        'm': torch.zeros( (ntilde) )\n",
    "        # 'm': torch.ones( (ntilde) )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Iteration*: 0\n",
      "No M-step\n",
      "final Loss: 3034.1894\n"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "theta_fit, f_params_fit, m_fit, V_fit, C_fit, mask_fit, K_tilde_fit, values_track_fit = varGP(X, r, **args)\n",
    "\n",
    "\n",
    "# The V_b that i get out of the A step is positive definite only because I add a 1.e-3 to the diagonal of the matrix\n",
    "\n",
    "# The problem is that this V_b is prernsteiojected onto a space that makes it not positive definite because before changing the space onto which i rpject, i go back to big V, which is ok not to be positive definite\n",
    "# if I then project this matrix though, it might result not positive definite on thenew space. I would need to find how V_b trasforms directly when i change K_tilde. \n",
    "# Probably its not possible and I shoudl just check if the dimension of the subspace its decreasing. If it is then probably the matrix is not positive definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model = {\n",
    "    'theta': theta,\n",
    "    'f_params': f_params,\n",
    "    'm': m,\n",
    "    'V': V,\n",
    "    'C': C,\n",
    "    'mask': mask,\n",
    "    'K_tilde_inv': K_tilde_inv,\n",
    "    'K_tilde': K_tilde,\n",
    "    'cellid': cellid,\n",
    "    'ntilde': ntilde,\n",
    "    'Maxiter': Maxiter,\n",
    "    'Nmstep': Nmstep,\n",
    "    'Nestep': Nestep,\n",
    "    'kernfun': kernfun,\n",
    "    'values_track': values_track,\n",
    "}\n",
    "\n",
    "# save_pickle('pietro_model', **model)\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# TEMP\n",
    "# To compare to samuele I have to kep the aplitude of C fixed to 1, he does not have it\n",
    "theta['Amp'] = 1.0\n",
    "\n",
    "\n",
    "print(f'f_params: {f_params}')\n",
    "for key, value in theta.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Predict and test\n",
    "\n",
    "rtst, R_predicted, r2, sigma_r2 = test(X_test, R_test, xtilde, **model )\n",
    "\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = fig.add_gridspec(5, 5,\n",
    "            left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "            wspace=0.3, hspace=0.7)\n",
    "dt = 0.05\n",
    "time_values = dt * np.arange( len(R_predicted) )\n",
    "ax = fig.add_subplot(gs[3:, :])\n",
    "ax.plot(time_values, np.mean(rtst, axis=0) / 0.05, 'k', linewidth=1)\n",
    "\n",
    "ax.plot(time_values, R_predicted / 0.05, color='red', label='GP')\n",
    "# ax.errorbar(time_values, R_predicted / 0.05, yerr=np.sqrt(sigma2_f[:,0].cpu()) / 0.05, color='red')\n",
    "# ax.legend(['data', 'GP'], loc='upper right', fontsize=14)\n",
    "txt = f'Pietro adjusted r^2 = {r2:.2f} ± {sigma_r2:.2f} Cell: {cellid}'\n",
    "ax.set_title(f'{txt}')\n",
    "# ax.set_ylabel('Firing rate (Hz)')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "a=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
